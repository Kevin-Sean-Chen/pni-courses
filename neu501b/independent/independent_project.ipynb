{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Robustness of Crayfish Nervous System to Environmental pH\n",
    "## Step 1: Format Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n"
     ]
    }
   ],
   "source": [
    "import neo, os\n",
    "import numpy as np\n",
    "from mne import create_info\n",
    "from mne.io import RawArray\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Define parameters.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "## I/O parameters.\n",
    "stop_list = ['171208_pH74_1_1.abf', '171208_pH72_1_1.abf']\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Main body.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "## Locate files.\n",
    "files = [f for f in os.listdir('raw') if f.endswith('abf')]\n",
    "files = [f for f in files if f not in stop_list]\n",
    "\n",
    "for f in files:\n",
    "\n",
    "    ## Load recordings.\n",
    "    recordings, = neo.AxonIO(filename='raw/%s' %f).read_block().segments\n",
    "    signal, = recordings.analogsignals\n",
    "\n",
    "    ## Concatenate raw recordings.\n",
    "    data = np.hstack([np.asarray(signal, dtype=np.float64)  * 1e-6]).T\n",
    "    if f.startswith('171212'): data *= -1 # Fix recording issue on Day 2.\n",
    "    \n",
    "    ## Create info object.\n",
    "    sfreq = float(signal.sampling_rate)\n",
    "    ch_names = ['nerve']\n",
    "    ch_types = 'bio'\n",
    "\n",
    "    info = create_info(ch_names=ch_names, sfreq=sfreq, ch_types=ch_types)\n",
    "\n",
    "    ## Create Raw object.\n",
    "    raw = RawArray(data, info, verbose=False)\n",
    "    \n",
    "    ## Save Raw object.  \n",
    "    raw.save('raw/%s' %f.replace('.abf','_raw.fif'), overwrite=True, verbose=False)\n",
    "    \n",
    "print('Done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f = '171208_pH74_1_1.abf'\n",
    "\n",
    "## Load recordings.\n",
    "recordings, = neo.AxonIO(filename='raw/%s' %f).read_block().segments\n",
    "signal, = recordings.analogsignals\n",
    "\n",
    "## Concatenate raw recordings.\n",
    "data = np.hstack([np.asarray(signal, dtype=np.float64)  * 1e-6]).T\n",
    "if f.startswith('171212'): data *= -1 # Fix recording issue on Day 2.\n",
    "\n",
    "## Create info object.\n",
    "sfreq = float(signal.sampling_rate)\n",
    "ch_names = ['nerve']\n",
    "ch_types = 'bio'\n",
    "\n",
    "info = create_info(ch_names=ch_names, sfreq=sfreq, ch_types=ch_types)\n",
    "\n",
    "## Create Raw object.\n",
    "raw = RawArray(data, info, verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from mne import Epochs, make_fixed_length_events\n",
    "from mne.io import Raw\n",
    "from pandas import DataFrame, concat\n",
    "from spike_sorting import find_threshold, peak_finder\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Define parameters.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "## I/O parameters.\n",
    "sessions = ['171208', '171212']\n",
    "\n",
    "## Filter parameters.\n",
    "l_freq = 300\n",
    "h_freq = 3000\n",
    "\n",
    "## Epoching.\n",
    "duration = 9.99 # seconds\n",
    "\n",
    "## Spike detection parameters.\n",
    "k = 5\n",
    "reject = 200 # uV\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Main loop.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "spikes = []\n",
    "for session in sessions:\n",
    "    \n",
    "    ## Locate files.\n",
    "    files = sorted([f for f in os.listdir('raw') if f.startswith(session) and f.endswith('fif')])\n",
    "    \n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "    ### Assemble recordings.\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "    \n",
    "    for f in files:\n",
    "    \n",
    "        ## Extract metadata.\n",
    "        session, pH, _, recording, _ = f.split('_')\n",
    "        pH = float(pH[-2:]) / 10\n",
    "    \n",
    "        #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "        ### Load and prepare data.\n",
    "        #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "        \n",
    "        ## Load raw.\n",
    "        raw = Raw('raw/%s' %f, preload=True, verbose=False)\n",
    "        \n",
    "        ## Filter data.\n",
    "        raw = raw.filter(l_freq, h_freq, picks=[0], method='fir', phase='zero', \n",
    "                         fir_design='firwin', verbose=False)\n",
    "        \n",
    "        ## Crop raw (remove filter artifact).\n",
    "        raw = raw.crop(0.05)\n",
    "        \n",
    "        #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "        ### Epoching.\n",
    "        #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "        \n",
    "        ## Make events of equal length.\n",
    "        events = make_fixed_length_events(raw, 1, duration=duration)\n",
    "        \n",
    "        ## Make epochs.\n",
    "        epochs = Epochs(raw, events, tmin=0, tmax=duration, baseline=None,\n",
    "                        picks=[0], preload=True, verbose=False)\n",
    "        \n",
    "        ## Extract data.\n",
    "        times = epochs.times\n",
    "        epochs = epochs.get_data().squeeze()\n",
    "        epochs *= 1e6 # Convert to uV.\n",
    "\n",
    "        #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "        ### Spike detection.\n",
    "        #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "        \n",
    "        ## Set threshold.\n",
    "        threshold = find_threshold(epochs, k)\n",
    "        \n",
    "        ## Iterate over epochs.\n",
    "        for i, epoch in enumerate(epochs):\n",
    "            \n",
    "            ## Detect spikes.\n",
    "            peak_loc, peak_mag = peak_finder(epoch, threshold)\n",
    "            if np.any(peak_loc): peak_loc = times[peak_loc] + i * times.max()\n",
    "    \n",
    "            ## Store as DataFrame. Append.\n",
    "            df = DataFrame( np.vstack([peak_loc, peak_mag]).T, columns=('Time','Amplitude') )\n",
    "            for column, value in zip(['Epoch','Recording','pH','Session'], [i+1, recording, pH, session]):\n",
    "                df.insert(0, column, value)\n",
    "            spikes.append(df)\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Assemble Spikes DataFrame.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "    \n",
    "## Concatenate spike DataFrames.\n",
    "spikes = concat(spikes)\n",
    "\n",
    "## Amplitude rejection.\n",
    "spikes = spikes[spikes.Amplitude < reject]\n",
    "    \n",
    "## Save.\n",
    "spikes.to_csv('spikes.csv', index=False)\n",
    "    \n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Assemble Counts DataFrame.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "    \n",
    "## Setup count GroupBy object.\n",
    "columns = ['Session','pH','Recording','Epoch']\n",
    "gb = spikes.groupby(columns)\n",
    "\n",
    "## Compute counts.\n",
    "counts = gb.Amplitude.count().reset_index()\n",
    "counts.columns = columns + ['Count']\n",
    "\n",
    "## Convert to Frequency.\n",
    "counts['Frequency'] = counts['Count'] / duration\n",
    "\n",
    "## Compute average spike amplitude.\n",
    "amplitude = gb.Amplitude.mean().reset_index()\n",
    "\n",
    "## Merge. Save.\n",
    "counts = counts.merge(amplitude, on=columns)\n",
    "\n",
    "## Save DataFrames.\n",
    "counts.to_csv('counts.csv', index=False)\n",
    "\n",
    "print('Done.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sanity Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from mne.io import Raw\n",
    "from spike_sorting import find_threshold, peak_finder\n",
    "sns.set_style('white')\n",
    "sns.set_context('notebook', font_scale=2)\n",
    "%matplotlib inline\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Define parameters.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "## I/O parameters.\n",
    "files = ['171212_ph%s_1_%s_raw.fif' %(pH,recording) for pH in [55,65,75] \n",
    "         for recording in np.arange(5)+1]\n",
    "\n",
    "## Filter parameters.\n",
    "l_freq = 300\n",
    "h_freq = 3000\n",
    "\n",
    "## Spike detection parameters.\n",
    "k = 5\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Main loop.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "for f in files:\n",
    "    \n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "    ### Load and prepare data.\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "    ## Load raw.\n",
    "    raw = Raw('raw/%s' %f, preload=True, verbose=False)\n",
    "\n",
    "    ## Filter data.\n",
    "    raw = raw.filter(l_freq, h_freq, picks=[0], method='fir', phase='zero', \n",
    "                     fir_design='firwin', verbose=False)\n",
    "\n",
    "    ## Crop raw (remove filter artifact).\n",
    "    raw = raw.crop(0.05)\n",
    "    \n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "    ### Plot.\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "    \n",
    "    ## Initialize canvas.\n",
    "    fig, axes = plt.subplots(3,1,figsize=(12,12))\n",
    "    \n",
    "    ## Define times for plotting.\n",
    "    tmax = raw.times.max()\n",
    "    time_limits = [(0,1), (tmax/2 - 0.5, tmax/2 + 0.5), (tmax-1,tmax)]\n",
    "    \n",
    "    ## Define threshold.\n",
    "    threshold = find_threshold(raw.get_data() * 1e6, k)\n",
    "    \n",
    "    for ax, tlim in zip(axes, time_limits):\n",
    "    \n",
    "        ## Extract data.\n",
    "        data, times = raw.get_data(start=raw.time_as_index(tlim[0])[0], \n",
    "                                   stop=raw.time_as_index(tlim[1])[0],\n",
    "                                   return_times=True)\n",
    "        data = data.squeeze() * 1e6\n",
    "        \n",
    "        ## Find spikes.\n",
    "        peak_loc, _ = peak_finder(data, threshold)\n",
    "        \n",
    "        ## Plot.\n",
    "        ax.plot(times, data, lw=2)\n",
    "        ax.hlines(threshold, *tlim)\n",
    "        ax.set(xlim=tlim, xlabel='Time (s)', ylim=(-100,100), ylabel=r'Amplitude ($\\mu V$)')\n",
    "        ax.set_title('%s spikes' %peak_loc.size, fontsize=24)\n",
    "        \n",
    "    ## Save.\n",
    "    sns.despine()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('qc/%s.png' %f.replace('_raw.fif', '.png'), dpi=180)\n",
    "    plt.close('all')\n",
    "    \n",
    "print('Done.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Statistics and Visualizations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spiking Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pandas import read_csv\n",
    "from statsmodels.api import OLS\n",
    "sns.set_style('white')\n",
    "sns.set_context('notebook', font_scale=2)\n",
    "%matplotlib inline\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Load data.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "counts = read_csv('counts.csv')\n",
    "counts['log_freq'] = np.log10(counts.Frequency)\n",
    "s1, s2 = counts[counts.Session==171208], counts[counts.Session==171212]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "result = OLS.from_formula('log_freq ~ pH', data=s1).fit()\n",
    "print(result.summary2())\n",
    "\n",
    "x1, x2 = s1.pH.min(), s1.pH.max()\n",
    "x = np.linspace(x1,x2,100)\n",
    "a, b = result.params\n",
    "A = np.power(10, a)\n",
    "y = A * np.power(10, b * x)\n",
    "plt.plot(x,y)\n",
    "plt.scatter(s1.pH, s1.Frequency, s=25, color='none', edgecolor='k')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "result = OLS.from_formula('log_freq ~ pH', data=s2).fit()\n",
    "print(result.summary2())\n",
    "\n",
    "x = np.linspace(2.5,7.5,100)\n",
    "a, b = result.params\n",
    "A = np.power(10, a)\n",
    "y = A * np.power(10, b * x)\n",
    "plt.plot(x,y)\n",
    "plt.scatter(s2.pH, s2.Frequency, s=25, color='none', edgecolor='k')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2,1,figsize=(12,8))\n",
    "\n",
    "## Plot Session 1.\n",
    "for ax, session, palette in zip(axes, counts.Session.unique(), ['Blues','Greens']):\n",
    "    sns.boxplot('pH', 'Frequency', data=counts[counts.Session==session], \n",
    "                palette=sns.color_palette(palette), ax=ax)\n",
    "    sns.swarmplot('pH', 'Frequency', data=counts[counts.Session==session], \n",
    "                  size=3, color='0.3', ax=ax)\n",
    "\n",
    "sns.despine()\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spikes = read_csv('spikes.csv')\n",
    "fig, axes = plt.subplots(2,6,figsize=(24,6), sharex=True, sharey=True)\n",
    "\n",
    "for i, session in enumerate(spikes.Session.unique()):\n",
    "    \n",
    "    for j, pH in enumerate(spikes.loc[spikes.Session==session,'pH'].unique()):\n",
    "        \n",
    "        ix = np.logical_and(spikes.Session==session, spikes.pH==pH)\n",
    "        sns.distplot(spikes.loc[ix, 'Amplitude'], kde=False, ax=axes[i,j])\n",
    "        \n",
    "axes[0,0].set(xlim=(18,100),xlabel='')\n",
    "\n",
    "sns.despine()\n",
    "plt.tight_layout()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
