{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Robustness of Crayfish Nervous System to Environmental pH\n",
    "## Step 1: Format Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n"
     ]
    }
   ],
   "source": [
    "import neo, os\n",
    "import numpy as np\n",
    "from mne import create_info\n",
    "from mne.io import RawArray\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Define parameters.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "## I/O parameters.\n",
    "stop_list = ['171208_pH74_1_1.abf', '171208_pH72_1_1.abf']\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Main body.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "## Locate files.\n",
    "files = [f for f in os.listdir('raw') if f.endswith('abf')]\n",
    "files = [f for f in files if f not in stop_list]\n",
    "\n",
    "for f in files:\n",
    "\n",
    "    ## Load recordings.\n",
    "    recordings, = neo.AxonIO(filename='raw/%s' %f).read_block().segments\n",
    "    signal, = recordings.analogsignals\n",
    "\n",
    "    ## Concatenate raw recordings.\n",
    "    data = np.hstack([np.asarray(signal, dtype=np.float64)  * 1e-6]).T\n",
    "    if f.startswith('171212'): data *= -1 # Fix recording issue on Day 2.\n",
    "    \n",
    "    ## Create info object.\n",
    "    sfreq = float(signal.sampling_rate)\n",
    "    ch_names = ['nerve']\n",
    "    ch_types = 'bio'\n",
    "\n",
    "    info = create_info(ch_names=ch_names, sfreq=sfreq, ch_types=ch_types)\n",
    "\n",
    "    ## Create Raw object.\n",
    "    raw = RawArray(data, info, verbose=False)\n",
    "\n",
    "    ## Save Raw object.  \n",
    "    raw.save('raw/%s' %f.replace('.abf','_raw.fif'), overwrite=True, verbose=False)\n",
    "    \n",
    "print('Done.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Session = 171208\n",
      "-----------------\n",
      "Preprocessing data\n",
      "  Data = 35 recordings, 12 epochs, 9.99s\n",
      "Spike Detection\n",
      "  Threshold = 17.60 uV\n",
      "  N spikes = 77124\n",
      "\n",
      "Session = 171212\n",
      "-----------------\n",
      "Preprocessing data\n",
      "  Data = 39 recordings, 12 epochs, 9.99s\n",
      "Spike Detection\n",
      "  Threshold = 18.06 uV\n",
      "  N spikes = 160525\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from mne import Epochs, make_fixed_length_events\n",
    "from mne.io import Raw\n",
    "from pandas import DataFrame, concat\n",
    "from spike_sorting import find_threshold, peak_finder\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Define parameters.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "## I/O parameters.\n",
    "sessions = ['171208', '171212']\n",
    "\n",
    "## Filter parameters.\n",
    "l_freq = 300\n",
    "h_freq = 3000\n",
    "\n",
    "## Epoching.\n",
    "duration = 9.99 # seconds\n",
    "\n",
    "## Spike detection parameters.\n",
    "k = 5\n",
    "reject = 200 # uV\n",
    "\n",
    "## Spike sorting parameters.\n",
    "n_clusters = 5\n",
    "n_refs = 10\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Main loop.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "dataset = []\n",
    "for session in sessions:\n",
    "    \n",
    "    ## Locate files.\n",
    "    files = sorted([f for f in os.listdir('raw') if f.startswith(session) and f.endswith('fif')])\n",
    "    msg = '\\nSession = %s' %session\n",
    "    print('%s\\n%s' %(msg,'-'*len(msg)))\n",
    "    \n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "    ### Assemble recordings.\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "    print('Preprocessing data')\n",
    "    \n",
    "    data = []\n",
    "    for f in files:\n",
    "        \n",
    "        #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "        ### Load and prepare data.\n",
    "        #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "        ## Load raw.\n",
    "        raw = Raw('raw/%s' %f, preload=True, verbose=False)\n",
    "        \n",
    "        ## Filter data.\n",
    "        raw = raw.filter(l_freq, h_freq, picks=[0], method='fir', phase='zero', \n",
    "                         fir_design='firwin', verbose=False)\n",
    "        \n",
    "        ## Crop raw (remove filter artifact).\n",
    "        raw = raw.crop(0.05)\n",
    "        \n",
    "        #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "        ### Epoching.\n",
    "        #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "        \n",
    "        ## Make events of equal length.\n",
    "        events = make_fixed_length_events(raw, 1, duration=duration)\n",
    "        \n",
    "        ## Make epochs.\n",
    "        epochs = Epochs(raw, events, tmin=0, tmax=duration, baseline=None,\n",
    "                        picks=[0], preload=True, verbose=False)\n",
    "        \n",
    "        ## Append.\n",
    "        data.append( epochs.get_data().squeeze() )\n",
    "        \n",
    "    ## Concatenate recordings.\n",
    "    data = np.array(data)\n",
    "    data *= 1e6 # Convert to uV.\n",
    "    times = epochs.times\n",
    "    \n",
    "    ## Print metadata.\n",
    "    n_recordings, n_epochs, n_times = data.shape\n",
    "    print('  Data = %s recordings, %s epochs, %0.2fs' %(n_recordings, n_epochs, n_times / raw.info['sfreq']))\n",
    "    \n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "    ### Spike detection.\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "    print('Spike Detection')\n",
    "    \n",
    "    ## Set threshold.\n",
    "    threshold = find_threshold(data, k)\n",
    "    print('  Threshold = %0.2f uV' %threshold)\n",
    "    \n",
    "    ## Iterate over epochs.\n",
    "    spikes = []\n",
    "    for i, f in enumerate(files):\n",
    "        \n",
    "        for j in range(n_epochs):\n",
    "            \n",
    "            ## Extract metadata.\n",
    "            session, pH, _, recording, _ = f.split('_')\n",
    "            pH = float(pH[-2:]) / 10\n",
    "            \n",
    "            ## Detect spikes.\n",
    "            peak_loc, peak_mag = peak_finder(data[i,j], threshold)\n",
    "            if np.any(peak_loc): peak_loc = times[peak_loc] + j * times.max()\n",
    "            \n",
    "            ## Store as DataFrame. Append.\n",
    "            df = DataFrame( np.vstack([peak_loc, peak_mag]).T, columns=('Time','Amplitude') )\n",
    "            for column, value in zip(['Epoch','Recording','pH','Session'], [j+1, recording, pH, session]):\n",
    "                df.insert(0, column, value)\n",
    "            spikes.append(df)\n",
    "            \n",
    "    ## Concatenate DataFrames.\n",
    "    spikes = concat(spikes)\n",
    "    \n",
    "    ## Amplitude rejection.\n",
    "    spikes = spikes[spikes.Amplitude < reject]\n",
    "    print('  N spikes = %s' %spikes.shape[0])\n",
    "    \n",
    "    ## Append.\n",
    "    dataset.append(spikes)\n",
    "    \n",
    "## Concatenate DataFrames.\n",
    "spikes = concat(dataset)\n",
    "spikes.to_csv('spikes.csv', index=False)\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Compute counts.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "## Setup GroupBy object.\n",
    "columns = ['Session','pH','Recording','Epoch']\n",
    "gb = spikes.groupby(columns)\n",
    "\n",
    "## Compute counts.\n",
    "counts = gb.Amplitude.count().reset_index()\n",
    "counts.columns = columns + ['Count']\n",
    "\n",
    "## Compute average spike amplitude.\n",
    "amplitude = gb.Amplitude.mean().reset_index()\n",
    "\n",
    "## Merge. Save.\n",
    "counts = counts.merge(amplitude, on=columns)\n",
    "counts.to_csv('counts.csv', index=False)\n",
    "\n",
    "print('Done.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
