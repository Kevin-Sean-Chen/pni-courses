{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, pickle, pystan, time\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from mne import read_label, read_surface\n",
    "from scripts.utilities import assemble_advi_params, tris_to_adj\n",
    "sns.set_style('white')\n",
    "sns.set_context('notebook', font_scale=1.5)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 1: Preparations\n",
    "## Load and prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Load data.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "## Load surface / label data.\n",
    "_, tris = read_surface('data/visual/lh.white')\n",
    "label = read_label('data/visual/lh.Medial_wall.label')\n",
    "\n",
    "## Load fMRI data.\n",
    "Y = np.load('data/visual/sub-01_task-visualcontrol_space-fsaverage5.L.psc.npz')['psc']\n",
    "_, total_vert = Y.shape\n",
    "\n",
    "## Load regressors.\n",
    "X = np.loadtxt('data/visual/sub-01_task-visualcontrol_events.txt')\n",
    "if X.ndim == 1: X = X.reshape(-1,1)\n",
    "\n",
    "Z = np.loadtxt('data/visual/sub-01_task-visualcontrol_motion.txt')\n",
    "if Z.ndim == 1: Z = Z.reshape(-1,1)\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Prepare data.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "## Remove triangles with vertices in the medial wall.\n",
    "ix = np.any(np.apply_along_axis(np.in1d, 0, tris, label.vertices), axis=-1)\n",
    "tris = tris[np.invert(ix)]\n",
    "\n",
    "## Make adjacency matrix.\n",
    "A = tris_to_adj(tris, remap_vertices=True, verbose=False).tocsr()\n",
    "\n",
    "## Compute degree.\n",
    "D = np.asarray(A.sum(axis=0)).squeeze()\n",
    "\n",
    "## Remove vertices in medial wall.\n",
    "ix = np.in1d(np.arange(total_vert), label.vertices)\n",
    "Y = Y[:,np.invert(ix)]\n",
    "\n",
    "## Define metadata.\n",
    "T, V = Y.shape\n",
    "T, K = X.shape\n",
    "T, M = Z.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Stan Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Mass Univariate Model.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "## Define path.\n",
    "f = 'stan_models/MassUnivariateAR.pkl'\n",
    "\n",
    "if not os.path.isfile(f):\n",
    "    \n",
    "    ## Compile StanModel.\n",
    "    MassUnivariate = pystan.StanModel(file=f.replace('pkl','stan'))\n",
    "    \n",
    "    ## Dump StanModel to file.\n",
    "    with open(f, 'wb') as f: pickle.dump(MassUnivariate, f)\n",
    "        \n",
    "else:\n",
    "    \n",
    "    ## Load StanModel.\n",
    "    MassUnivariate = pickle.load(open(f,'rb'))\n",
    "    \n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Unweighted Graph Laplacian (IAR) Model.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "## Define path.\n",
    "f = 'stan_models/IAR.pkl'\n",
    "\n",
    "if not os.path.isfile(f):\n",
    "    \n",
    "    ## Compile StanModel.\n",
    "    IAR = pystan.StanModel(file=f.replace('pkl','stan'))\n",
    "    \n",
    "    ## Dump StanModel to file.\n",
    "    with open(f, 'wb') as f: pickle.dump(MassUnivariate, f)\n",
    "        \n",
    "else:\n",
    "    \n",
    "    ## Load StanModel.\n",
    "    IAR = pickle.load(open(f,'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2: Estimation\n",
    "### Mass Univariate\n",
    "The simplest model is the standard model in fMRI: mass univariate. In mass univariate, all regression coefficients are estimated independently of one another. \n",
    "\n",
    "For a single subject, we have a set of observations $Y \\in \\mathbb{R}^{T,V}$, where $T$ is the number of time points and $V$ is the number of voxels. We also have a set of task regressors observations $X \\in \\mathbb{R}^{T,K}$ and nuisance regressors $Z \\in \\mathbb{R}^{T,M}$, where $K$ and $M$ are the number of task and nuisance regressors, respectively. We assume that the observed BOLD signal is a linear combination of the weighted sets of task and nuisance regressors:\n",
    "\n",
    "$$ Y = XB + ZG + \\epsilon $$\n",
    "\n",
    "where $B \\in \\mathbb{R}^{K,V}$ and $G \\in \\mathbb{R}^{M,V}$ are the regression weights mapping the task and nuisance regressors, respectively. We assume the errors are normally distributed such that:\n",
    "\n",
    "$$ \\epsilon \\sim \\mathcal{N}(0, \\Sigma) $$\n",
    "\n",
    "$$ Y \\sim \\mathcal{N}(XB + ZG, \\Sigma) $$\n",
    "\n",
    "where $\\Sigma \\in \\mathbb{R}^{V,V}$ is the covariance matrix. If we assume *independent and identically distributed (iid)* residuals, then the covariance matrix is equivalent to $\\Sigma = \\sigma^2 I$. \n",
    "\n",
    "Unfortunately fMRI data violates the *iid* assumptions due to temporally correlated residuals as a result of non-neural contributions to the BOLD signal (e.g. respiratory, scanner noise). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Prepare data.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "## Prepare data.\n",
    "data = dict(T=T, V=V, K=K, M=M, AR=1, Y=Y, X=X, Z=Z)\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Compute MAP estimates.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "## MAP estimates used as starting point for\n",
    "## ADVI sampling.\n",
    "\n",
    "st = time.time()\n",
    "MAP = MassUnivariate.optimizing(data=data, seed=47404)\n",
    "print('MAP elapsed time: %0.1f s' %(time.time() - st))\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Compute VB estimates.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "st = time.time()\n",
    "VB = MassUnivariate.vb(data=data, init=MAP, output_samples=1000, seed=47404)\n",
    "print('VB elapsed time: %0.1f s' %(time.time() - st))\n",
    "\n",
    "## Reassemble outputs.\n",
    "param_names = ['B','G','sigma']\n",
    "dim = [[K,V],[M,V],[1]]\n",
    "params = assemble_advi_params(VB['sampler_params'], param_names, dim)\n",
    "\n",
    "## TODO\n",
    "# 1) Dump data to file\n",
    "# 2) compute excursion set\n",
    "# 3) compute masked overlay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unweighted Graph Laplacian (IAR)\n",
    "\n",
    "The simplest model of spatial dependence we will deal with is a simple smoothing prior on the regression weights $\\vec{w}$ of length $V$:\n",
    "\n",
    "$$ p(\\vec{w} \\mid \\tau^{-1}, L^{-1}) = \\mathcal{N}(0, \\tau^{-1} L^{-1} ) $$\n",
    "\n",
    "where $L$ is the **unweighted graph laplacian (UGL)**, defined as:\n",
    "\n",
    "$$ L = D - A $$\n",
    "\n",
    "where $D$ is a $VxV$ diagonal matrix where values $d_{i,i}$ is the *degree* of the corresponding vertex, and $A$ is the adjacency matrix of the mesh. Concretely, the UGL matrix can be defined as:\n",
    "\n",
    "$$ L_{i,j} = \\begin{cases} d_{i} & \\text{if} \\ i=j \\\\ -1 & \\text{if} \\ adj(i,j) \\\\ 0 & \\text{otherwise} \\end{cases} $$\n",
    "\n",
    "The UGL matrix can be interpreted as a differencing matrix; in other words, the values of $\\vec{w}$ are penalized for their differences from their neighbors. \n",
    "\n",
    "The UGL is worth considering as it is popular and the default prior for Bayesian analysis in SPM.\n",
    "\n",
    "Formally, we can express the prior above as:\n",
    "\n",
    "$$ p(\\vec{w} \\mid \\tau^{-1}, L^{-1}) = \\frac{1}{\\sqrt{2 \\pi^n \\left| \\tau^{-1} L^{-1} \\right|}} \\text{exp}\\left(-\\frac{1}{2}\\vec{w}^T (\\tau L) \\vec{w}\\right)$$\n",
    "\n",
    "We will take the log-likelihood:\n",
    "\n",
    "$$ \\text{log} \\ p(\\vec{w} \\mid \\tau^{-1}, L^{-1})= -\\frac{1}{2} \\left( n\\text{log}(2 \\pi) + \\text{log}\\left(\\left| \\tau^{-1} L^{-1} \\right|\\right) + \\vec{w}^T (\\tau L) \\vec{w} \\right) $$\n",
    "\n",
    "Drop constant:\n",
    "\n",
    "$$ =  -\\frac{1}{2}\\text{log}\\left(\\left| \\tau^{-1} L^{-1} \\right|\\right) -\\frac{1}{2} \\vec{w}^T (\\tau L) \\vec{w} $$\n",
    "\n",
    "Factor out $\\tau$:\n",
    "\n",
    "$$ = -\\frac{1}{2}\\text{log}\\left(\\tau^{-n} \\left| L^{-1} \\right|\\right) -\\frac{1}{2} \\vec{w}^T (\\tau L) \\vec{w} $$\n",
    "\n",
    "$$ = \\frac{n}{2}\\text{log}(\\tau) - \\frac{1}{2}\\text{log}\\left( \\left| L^{-1} \\right|\\right) -\\frac{1}{2} \\vec{w}^T (\\tau L) \\vec{w} $$\n",
    "\n",
    "$\\tau L$ is sparse, so matrix multiplication will be quick. That said, its inverse determinant is two $O(n^3)$ computations. However, [Jin, Carlin, and Banerjee (2005)](https://www.ncbi.nlm.nih.gov/pubmed/16401268) show that:\n",
    "\n",
    "$$ \\mid L \\mid = \\mid D - A \\mid \\propto \\prod_{i=1}^{n}(1 - \\lambda_i) $$\n",
    "\n",
    "where $\\lambda_1, ..., \\lambda_i$ are the eigenvalues of $D^{-\\frac{1}{2}}WD^{-\\frac{1}{2}}$, which can be computed ahead of time and passed in as data. Finally, we know that $\\text{det}(A^{-1}) = \\frac{1}{\\text{det}(A)}$, therefore:\n",
    "\n",
    "$$ = \\frac{n}{2}\\text{log}(\\tau) - \\frac{1}{2}\\text{log}\\left( \\frac{1}{\\prod_{i=1}^{n}(1 - \\lambda_i)} \\right) -\\frac{1}{2} \\vec{w}^T (\\tau L) \\vec{w} $$\n",
    "\n",
    "Notably, the product of the eigenvalues just becomes another constant and can therefore be dropped too. So we end up with:\n",
    "\n",
    "$$ = \\frac{n}{2}\\text{log}(\\tau) -\\frac{1}{2} \\vec{w}^T (\\tau L) \\vec{w} $$\n",
    "\n",
    "Very sparse!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Prepare data.\n",
    "data = dict(T=T, V=V, K=K, M=M, nz = A.data.size, Y=Y, X=X, Z=Z, D=D, \n",
    "            Aw=A.data, Av=A.indices+1, Au=A.indptr+1)\n",
    "\n",
    "## Estimate MAP.\n",
    "st = time.time()\n",
    "MAP = IAR.optimizing(data=data, seed=47404, verbose=True)\n",
    "print('Elapsed time: %s s' %(time.time() - st))\n",
    "\n",
    "## Save overlay.\n",
    "B = np.zeros_like(ix, dtype=float)\n",
    "B[np.invert(ix)] = MAP['B'].squeeze()\n",
    "obj = nib.Nifti1Image(B.reshape(-1,1,1,1), np.identity(4))\n",
    "nib.save(obj, 'iar_L.nii.gz')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
