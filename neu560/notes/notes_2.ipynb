{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neu 560 (2018-02-13): SVD and Linear Systems\n",
    "\n",
    "## Applications of SVD\n",
    "\n",
    "### Rank\n",
    "The **rank** of a matrix is defined as the number of linearly independent columns or rows (these are always the same). For a $m \\ x \\ n$ matrix, the rank must be:\n",
    "> $\\text{rank} \\leq \\text{min}(m, n)$. \n",
    "\n",
    "Equivalently, the rank of a matrix $A$ is equal to the number of non-zero singular values. As such, for a matrix $A = USV^T$, the following is true:\n",
    "\n",
    "> 1. The first $k$ left singular vectors, $\\{u_1, u_2, \\ldots, u_k\\}$, provide an orthonormal basis for the column space of $A$.\n",
    "\n",
    "> 2. The first $k$ right singular vectors, $\\{v_1, v_2, \\ldots, v_k\\}$, provide an orthonormal basis for the row space of $A$.\n",
    "\n",
    "> 3. The last right singular vectors, $\\{v_{k+1}, v_{k+2}, \\ldots, v_n\\}$, provide an orthonormal basis for the null space of $A$.\n",
    "\n",
    "### Positive semidefinite matrix\n",
    "A **positive semi-definite (PSD) matrix** is a matrix that has all eigenvalues $\\geq 0$, or equivalently, a matrix $A$ for which $x^TAx \\geq 0$ for any vector $x$.\n",
    "\n",
    "A PSD matrix can be made from any matrix $X$ and let $A = X^TX$.\n",
    "\n",
    "### Relationship between SVD and eigenvector decomposition\n",
    "The eigenvector of a square matrix $A$ is defined as a vector satisfying the equation:\n",
    "\n",
    "> $Ax = \\lambda x$\n",
    "\n",
    "where $\\lambda$ is the corresponding eigenvalue. In other words, an eigenvector of $A$ is any vector that, when multiplied by $A$, comes back as itself scaled by $\\lambda$.\n",
    "\n",
    "When a matrix $A$ is symmetric and positive semi-definite, then the SVD is equivalent to eigendecomposition:\n",
    "\n",
    "> $A = USU^T$\n",
    "\n",
    "where the columns of $U$ are eigenvectors and the diagonal entries $\\{ s_i \\}$ of $S$ are the eigenvalues.\n",
    "\n",
    "Notably, if $A = USV^T$, then the SVD of $A^TA$ is equivalent to:\n",
    "\n",
    "> $A^TA = VS^2V^T$\n",
    "\n",
    "## Least Squares Regression\n",
    "The challenge is to identify some set of weights, $w$, that allows us to maximally predict some observations, $y$, with some predictors, $x$:\n",
    "\n",
    "> $ y \\approx wx$\n",
    "\n",
    "To do so, we try to minimize the squared prediction error. In other words, we attempt to find the vector weights $w$ that minimizes:\n",
    "\n",
    "> $\\text{squared error} = \\sum(y_i - x_i \\cdot w)^2$\n",
    "\n",
    "It turns out that the vector that minimizes the above squared error can be estimated as:\n",
    "\n",
    "> $w = (X^TX)^{-1}(X^TY)$\n",
    "\n",
    "### Proof 1: Orthogonality\n",
    "Think about the predictors, $X$, as a matrix of columns. The columns of $X$ span a $d$-dimensional subspace within the larger $N$-dimensional vector space that contains the vector $Y$. Least squares regression is trying to find the linear combination of these vectors, $Xw$, that gets as close as possible to $Y$.\n",
    "\n",
    "The optimal linear combination is one that drops a line down from $Y$ to the subspace spanned by $\\{X_1, \\ldots, X_D\\}$ at a right angle. In other words, the **residual error** should be orthogonal to every column of X:\n",
    "\n",
    "> $(Y - Xw) \\cdot X_j = 0$\n",
    "\n",
    "With this in mind, we can solve for $\\overrightarrow w$:\n",
    "\n",
    "> $X^T(Y - Xw) = 0$\n",
    "\n",
    "> $X^TY - X^TXw = 0$\n",
    "\n",
    "> $X^TY = X^TXw$\n",
    "\n",
    "> $w = (X^TX)^{-1}(X^TY)$\n",
    "\n",
    "### Proof 2: Calculus\n",
    "First we need to define two useful identities:\n",
    "\n",
    "1) Derivative of a linear function:\n",
    "\n",
    "> $\\nabla \\overrightarrow a \\cdot \\overrightarrow x = \\nabla \\overrightarrow a^T \\overrightarrow x = \\nabla \\overrightarrow x^T \\overrightarrow a = \\overrightarrow a$\n",
    "\n",
    "2) Derivative of a quadratic function: \n",
    "\n",
    "> If $A$ is symmetric: $\\nabla xAx = 2Ax$\n",
    "\n",
    "> If $A$ is non-symmetric: $\\nabla xAx = (A+A^T)x$\n",
    "\n",
    "With these identities in mind, we return to linear regression. In linear regression, we are finding the set of weights, $w$, that minimize the squared vector norm of the difference between the weighted predictors and observed variables:\n",
    "\n",
    "> $\\text{squared error} = \\lVert Y - Xw \\rVert^2 = (Y - Xw)^T(Y-Xw)$\n",
    "\n",
    "We can take the derivative of this expression with respect to $w$ to find the set of $w$ that minimizes it:\n",
    "\n",
    "> $ \\nabla (Y - Xw)^T(Y-Xw) = 0$\n",
    "\n",
    "> $ \\nabla (Y^TY - w^TX^TY - w^TX^TY + w^TX^TXw) = 0$\n",
    "\n",
    "> $ \\nabla (Y^TY - 2w^TX^TY + w^TX^TXw) = 0$\n",
    "\n",
    "> $ 0 - 2X^TY + 2X^TXw = 0$\n",
    "\n",
    "> $ 2X^TXw = 2X^TY$\n",
    "\n",
    "> $ w = (X^TX)^{-1}(X^TY)$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
