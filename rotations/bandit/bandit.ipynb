{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial on Reinforcement Learning\n",
    "Inspired by [Daw (2009)](http://www.oxfordscholarship.com/view/10.1093/acprof:oso/9780199600434.001.0001/acprof-9780199600434-chapter-001).\n",
    "\n",
    "## Section 1: A Simple Reinforcement Learning Model\n",
    "In the simplest scenario, we can imagine a decision-making agent deciding between two arms (left and right) of a noisy slot machine (i.e. a two-arm bandit problem). The slot machine returns rewards stochastically as follows: $\\bar r_L = \\$1$ and $\\bar r_R = \\$0$. \n",
    "\n",
    "We can model how this agent learns about the rewards, $r$, with each choice, $c$, across trials, $t$, using the **Q-learning model** ([Watkins, 1989](https://s3.amazonaws.com/academia.edu.documents/50360235/Learning_from_delayed_rewards_20161116-28282-v2pwvq.pdf?AWSAccessKeyId=AKIAIWOWYYGZ2Y53UL3A&Expires=1509845344&Signature=iVnCn0Aak3hwqransm718DmQG6o%3D&response-content-disposition=inline%3B%20filename%3DLearning_from_delayed_rewards.pdf)). In this model, we assume that, on each trial $t$, the agent assigns an expected value to each arm $Q_t(L)$ and $Q_t(R)$, and makes a choice, $c_t$, based on these values. Following the trial outcome, the value of the choice is updated:\n",
    "\n",
    "> $Q_{t+1}(c_t) = Q_{t}(c_t) + \\alpha \\cdot \\delta_t$\n",
    "\n",
    "where $\\alpha$ is the learning parameter on the range of $[0,1]$, and $\\delta_t$ is the prediction error:\n",
    "\n",
    "> $\\delta_t = r_t - Q_{t}(c_t)$\n",
    "\n",
    "To explain the choices, $c_t$, in terms of the values, $Q_t$, we assume an *observation model*, which relates the latent variables to the observed data. In reinforcement learning, it is frequently assumed subjects choose probabilistically according to a *softmax distribution*:\n",
    "\n",
    "> $ p( c_t = L \\ | \\ Q_{t}(L), Q_{t}(R) ) = \\frac{exp( \\beta \\cdot Q_{t}(L) )}{exp( \\beta \\cdot Q_{t}(L) ) + exp( \\beta \\cdot Q_{t}(R) )}$\n",
    "\n",
    "Here, $\\beta$ is the *inverse temperature* parameter. When there are only two choices, the formula above is equivalent to logistic regression on binary choice, $c_t$, predicated on the difference in values, $Q_{t}(L) - Q_{t}(R)$. As such, $\\beta$ can equivalently be understood as the regression weight linking the values, $Q$ to the choices, $c$. When there are more than two choice options, the softmax corresponds to a generalization of logistic regression (i.e. conditional logit regression).\n",
    "\n",
    "[It is worth mentioning that the formulas above are the foundation for a variety of reinforcement learning models, but by no means cover the complexity of models in the literature. Similarly, Q-learning can be expanded upon so as to, for example, allow for time-varying learning parameters.]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Programming the N-Arm Bandit Problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style('white')\n",
    "sns.set_context('notebook', font_scale=2)\n",
    "%matplotlib inline\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Define useful functions.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "def logistic(arr):\n",
    "    return 1. / (1 + np.exp(-arr))\n",
    "\n",
    "def softmax(arr, i, beta=1):\n",
    "    return np.exp(beta * arr[i]) / np.sum( np.exp( beta * arr ) )\n",
    "\n",
    "def fixed_bandit(alpha, beta, size=100, params=([1,0], [0.1,0.1])):    \n",
    "        \n",
    "    ## Preallocate rewards.\n",
    "    mu, sd = params\n",
    "    rewards = np.random.multivariate_normal(mu, np.diag(sd), size)\n",
    "\n",
    "    ## Preallocate space.\n",
    "    Q, theta, choice = [np.zeros_like(rewards) for _ in np.arange(3)]\n",
    "    \n",
    "    ## Main loop.\n",
    "    for i in np.arange(size-1):\n",
    "        \n",
    "        ## Action selection.\n",
    "        theta[i]  = [softmax(Q[i], j, beta=beta) for j in np.arange(theta.shape[-1])]\n",
    "        choice[i] = np.random.multinomial(1, theta[i])\n",
    "        \n",
    "        ## Value updating.\n",
    "        j = np.argmax(choice[i])\n",
    "        Q[i+1, j] = Q[i,j] + alpha * (rewards[i,j] - Q[i,j])\n",
    "        \n",
    "    return Q, theta, choice\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Run simulation.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "np.random.seed(18940)\n",
    "\n",
    "## Define parameters.\n",
    "alpha = 0.1\n",
    "beta = 5\n",
    "\n",
    "## Simulate 2-arm bandit RL.\n",
    "Q, theta, choice = fixed_bandit(alpha, beta, size=50)\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Plot.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "## Initialize canvas.\n",
    "fig, ax = plt.subplots(1,1,figsize=(12,4))\n",
    "\n",
    "## Plot.\n",
    "ax.plot(np.arange(50)+1, Q[:,0], linewidth=2, label=r'$Q_L$')\n",
    "ax.plot(np.arange(50)+1, Q[:,1], linewidth=2, label=r'$Q_R$')\n",
    "ax.hlines([0,1], 0, 101, linestyle='--', alpha=0.5)\n",
    "for i, color in enumerate(sns.color_palette(n_colors=2)):\n",
    "    x, = np.where(choice[:,i] == 1)\n",
    "    ax.scatter(x, np.ones_like(x)*1.5, s=25, marker='o', color=color)\n",
    "\n",
    "## Add information.\n",
    "ax.set(xlim=(0.5,51), xlabel='Trial', ylim=(-0.1,1.55), ylabel=r'$Q$ Value')\n",
    "ax.legend(loc=1, borderpad=0.5, labelspacing=0)\n",
    "ax.text(51, 1, r'$R_L$', ha='left', va='center', fontsize=20)\n",
    "ax.text(51, 0, r'$R_R$', ha='left', va='center', fontsize=20)\n",
    "\n",
    "sns.despine()\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding the softmax distribution\n",
    "First let's demonstrate that the softmax is a generalization of the logistic regression. We will compare the likelihood-of-take, $\\theta_L$, from the softmax and logistic transforms for a variety of $Q_L$ and $Q_R$ values. Remember that, for the logistic, choice is predicated on the difference of the two values: $Q_L - Q_R$.\n",
    "\n",
    "**Note:** In the example below, the inverse temperature parameter, $\\beta$, is fixed at 1. We will cover the effects of changing this parameter below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Compute likelihood-of-take.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "## Simulate pairs of Q-values.\n",
    "q1 = q2 = np.linspace(-5,5,11,dtype=int)\n",
    "q = np.array(np.meshgrid(q1,q2)).reshape(2,11**2)\n",
    "\n",
    "## Estimate theta.\n",
    "theta_L = logistic( q[0] - q[1] )\n",
    "theta_S = np.apply_along_axis(softmax, 0, q, i=0)\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Plot.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "## Initialize Canvas.\n",
    "fig, axes = plt.subplots(1,2,figsize=(16,6))\n",
    "\n",
    "## Plot logistic.\n",
    "sns.heatmap(theta_L.reshape(11,11), vmin=0, vmax=1, square=True, \n",
    "                 cbar_kws=dict(label=r'Likelihood of Take ($\\theta_L$)'), ax=axes[0])\n",
    "axes[0].set(xticklabels=q1, xlabel=r'$Q_L$ values', yticklabels=q2,  ylabel=r'$Q_R$ values', title='Logistic')\n",
    "axes[0].invert_yaxis()\n",
    "\n",
    "## Plot softmax.\n",
    "sns.heatmap(theta_S.reshape(11,11), vmin=0, vmax=1, square=True, \n",
    "                 cbar_kws=dict(label=r'Likelihood of Take ($\\theta_L$)'), ax=axes[1])\n",
    "axes[1].set(xticklabels=q1, xlabel=r'$Q_L$ values', yticklabels=q2,  ylabel=r'$Q_R$ values', title='Softmax')\n",
    "axes[1].invert_yaxis()\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! We can verify empirically the relationship between the logistic and softmax distributions. But why use the softmax distribution? It has a number of desirable properties useful for model estimation and fitting:\n",
    "- Normalizes values between $[0,1]$. This is especially useful as it allows Q-values to range $[-\\infty, \\infty]$.\n",
    "- Sigmoid shape preserves the significance of values within 1 standard deviation of the mean.\n",
    "- Sigmoid shape removes the influence of outlier data with smooth, monotonic nonlinearity at both extremes.\n",
    "- Differentiable at every point on the curve.\n",
    "\n",
    "We demonstrate the features of the softmax function below in examples of a 2-arm and 3-arm bandit, where the expected reward, $Q_n$, last arm is held constant at 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### 2d plot.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "## Simulate pairs of Q-values.\n",
    "q = np.zeros((2,101))\n",
    "q[0] = np.linspace(-5,5,101)\n",
    "\n",
    "## Estimate theta.\n",
    "theta = np.apply_along_axis(softmax, 0, q, i=0)\n",
    "\n",
    "## Plot.\n",
    "fig, axes = plt.subplots(1,2,figsize=(12,4))\n",
    "axes[0].plot(q[0], theta)\n",
    "axes[0].set(xlabel=r'$Q_1$ values', ylabel=r'$\\theta_1$', title=r'$Q_2=0$')\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### 3d plot.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "## Simulate triples of Q-values.\n",
    "q = np.linspace(-5,5,101)\n",
    "q = np.array(np.meshgrid(q,q)).reshape(2,101**2)\n",
    "q = np.concatenate([q, np.zeros((1,q.shape[-1]))], axis=0)\n",
    "\n",
    "## Estimate theta.\n",
    "theta = np.apply_along_axis(softmax, 0, q, i=0)\n",
    "\n",
    "## Plot.\n",
    "sns.heatmap(theta.reshape(101,101), vmin=0, vmax=1, \n",
    "            cbar_kws=dict(label=r'$\\theta_1$'), ax=axes[1])\n",
    "axes[1].set(xticks=np.arange(0,101,10), xticklabels=np.linspace(-5,5,11,dtype=int), xlabel=r'$Q_1$ values', \n",
    "            yticks=np.arange(0,101,10), yticklabels=np.linspace(-5,5,11,dtype=int), ylabel=r'$Q_2$ values', \n",
    "            title=r'$Q_3=0$')\n",
    "axes[1].invert_yaxis()\n",
    "\n",
    "sns.despine(ax=axes[0])\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding the inverse temperature parameter ($\\beta$)\n",
    "The inverse temperature parameter, $\\beta$, can be thought to modulate the slope of the softmax distribution. Higher values of $\\beta$ increase the gain of the function, such that smaller relative differences between Q-values (i.e. $Q_L$ > $Q_R$) are needed to bias decision making towards $Q_L$ to the same degree. \n",
    "\n",
    "This is demonstrated empirically below for a 2-arm bandit problem (as described above) varying the values of $Q_L$, but holding constant $Q_R=0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Define parameters.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "## Define parameters.\n",
    "betas = [0.5, 1, 2, 5, 10, 20]\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Main loop.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "## Simulate pairs of Q-values.\n",
    "q = np.zeros((2,101))\n",
    "q[0] = np.linspace(-5,5,101)\n",
    "\n",
    "## Initialize canvas.\n",
    "fig, ax = plt.subplots(1,1,figsize=(12,4))\n",
    "\n",
    "for beta in betas:\n",
    "\n",
    "    ## Estimate theta.\n",
    "    theta = np.apply_along_axis(softmax, 0, q, i=0, beta=beta)\n",
    "    \n",
    "    ## Plot.\n",
    "    ax.plot(q[0], theta, label=beta, alpha=0.75, linewidth=2)\n",
    "    \n",
    "## Add information to plot.\n",
    "ax.set(xlim=(-5,5), xticks=[-5,-2.5,0,2.5,5], xlabel=r'$Q_L$ values', ylabel=r'$\\theta_L$')\n",
    "ax.legend(loc=2, ncol=2, borderpad=0, labelspacing=0, columnspacing=0.8)\n",
    "\n",
    "sns.despine()\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding the learning parameter ($\\alpha$)\n",
    "The learning parameter, $\\alpha$, modulates the degree to which the reward prediction error (RPE) modulates the current value estimate. When $\\alpha$ is high, prediction errors are integrated rapidly with the value estimate over-representing the short-term history of rewards. When $\\alpha$ is low, prediction errors are integrated slowly with the value estimate representing both the short-term and long-term history of rewards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Define parameters.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "## Define parameters.\n",
    "alphas = [0.10,0.01,0.50]\n",
    "beta = 5\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Main loop.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "np.random.seed(18940)\n",
    "\n",
    "## Initialize canvas.\n",
    "fig, ax = plt.subplots(1,1,figsize=(12,4))\n",
    "\n",
    "for alpha, color in zip(alphas, ['#1f77b4','#2ca02c','#d62728']):\n",
    "    \n",
    "    ## Simulate 2-arm bandit RL.\n",
    "    Q, _, _ = fixed_bandit(alpha, beta, size=50)\n",
    "    \n",
    "    ## Plot.\n",
    "    ax.plot(np.arange(50)+1, Q[:,0], linewidth=2, label=r'$\\alpha_1=%0.2f$' %alpha, \n",
    "            color=color, alpha=0.75)\n",
    "\n",
    "## Add information.\n",
    "ax.hlines([0,1], 0, 101, linestyle='--', alpha=0.5)\n",
    "ax.set(xlim=(0.5,51), xlabel='Trial', ylim=(-0.1,1.55), ylabel=r'$Q_L$ Value')\n",
    "ax.legend(loc=7, bbox_to_anchor=(1.2,0.4), borderpad=0, labelspacing=0)\n",
    "ax.text(51, 1, r'$R_L$', ha='left', va='center', fontsize=20)\n",
    "ax.text(51, 0, r'$R_R$', ha='left', va='center', fontsize=20)\n",
    "\n",
    "sns.despine()\n",
    "plt.tight_layout()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "toc": {
   "nav_menu": {
    "height": "117px",
    "width": "252px"
   },
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
