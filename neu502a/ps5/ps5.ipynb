{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NEU502a (Spring 2018)\n",
    "## Problem Set #5: Reinforcement Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.io import loadmat\n",
    "from scipy.optimize import minimize\n",
    "sns.set_style('white')\n",
    "sns.set_context('notebook', font_scale=1.5)\n",
    "%matplotlib inline\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Define useful functions.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "def inv_logit(arr):\n",
    "    '''Elementwise inverse logit (logistic) function.'''\n",
    "    return 1 / (1 + np.exp(-arr))\n",
    "\n",
    "def phi_approx(arr):\n",
    "    '''Elementwise fast approximation of the cumulative unit normal. \n",
    "    For details, see Bowling et al. (2009). \"A logistic approximation \n",
    "    to the cumulative normal distribution.\"'''\n",
    "    return inv_logit(0.07056 * arr ** 3 + 1.5976 * arr)\n",
    "\n",
    "def softmax(arr, beta=1):\n",
    "    '''Softmax function'''\n",
    "    return np.exp(beta * arr) / np.nansum( np.exp( beta * arr ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# README\n",
    "I'm not 100% confident in these answers so far given ambiguities in Sutton & Barto, ch. 6/12. \n",
    "\n",
    "Also everything desperately needs to be rewritten and organized."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1\n",
    "### The Setup\n",
    "\n",
    "We have three states, ${S_1, S_2, S_3}$ and two actions ${A_1, A_2}$ where the state action pairs have a unique transition structure: $A_1 \\mid S_1 \\rightarrow S_2 $ and $A_2 \\mid S_1 \\rightarrow S_3 $. \n",
    "\n",
    "The challenge is to learn the value of each state-action pair so as to maximize reward over time. We will assign a new set of variables, $Q$ values, to represent these estimates: \n",
    "\n",
    "|       | $A_1$     | $A_2$     |\n",
    "|:-----:|:---------:|:---------:|\n",
    "| $S_1$ | $Q_{1,1}$ | $Q_{1,2}$ |\n",
    "| $S_2$ | $Q_{2,1}$ | $Q_{2,2}$ |\n",
    "| $S_3$ | $Q_{3,1}$ | $Q_{3,2}$ |\n",
    "\n",
    "where the notation denotes *[state, action]*. Note that these values will update over time with each additional task trial completed. \n",
    "\n",
    "### Model-Free Learning with SARSA-TD(0)\n",
    "\n",
    "The general TD(0) learning rule for a trial $t$:\n",
    "\n",
    "$$ \\delta =  R_t + \\gamma Q(s',a') - Q(s,a) $$\n",
    "\n",
    "$$ Q(s,a) = Q(s,a) + \\eta \\delta $$\n",
    "\n",
    "where $Q(s,a)$ is the value of the current state-action pair; $Q(s',a')$ is the value of the next state-action pair; $\\eta$ is the learning rate; and $\\gamma$ is a discounting factor. In the following, we will set the discounting parameter $\\gamma=1$ because the two-step task does not allow subjects to choose between rewards at different delays.\n",
    "\n",
    "1) At the final, or terminal, states we know that there is no next state-action pair. Thus, the formula simplifies to:\n",
    "\n",
    "$$ \\delta = R - Q(s,a) $$\n",
    "\n",
    "$$ Q(s,a) = Q(s,a) + \\eta \\delta $$\n",
    "\n",
    "2) At the initial states we know the agent does not experience reward. Thus the formula simplifies to:\n",
    "    \n",
    "$$ \\delta = Q(s',a') - Q(s,a) $$\n",
    "    \n",
    "$$ Q(s,a) = Q(s,a) + \\eta \\delta $$\n",
    "\n",
    "---\n",
    "\n",
    "### Model-Free Learning with SARSA-TD($\\lambda$)\n",
    "\n",
    "If we parameterize the model according with eligibility traces, then what we have is:\n",
    "\n",
    "$$ e(s,a) = \\begin{cases} \\lambda \\ e(s,a)_{t-1} & if s \\neq s_t \\\\ \\lambda \\ e(s,a)_{t-1} & if s = s_t \\\\ \\end{cases} $$\n",
    "\n",
    "where $e(s,a)_{t=0}=0$. At the terminal state we have:\n",
    "\n",
    "$$ \\delta = R - Q(s,a) $$\n",
    "\n",
    "and each $Q$-value updated such that:\n",
    "\n",
    "$$ Q(s,a) = Q(s,a) + \\eta \\delta e(s,a) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sarsa_ngl(params, Y, R, n_states=3, n_actions=2):\n",
    "    '''Negative log-likelihood function of SARSA-TD(lambda) model.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    params : list\n",
    "      SARSA model parameters, (eta, lambda, beta)\n",
    "    Y : 2d array\n",
    "      Choices of participant (see notes).\n",
    "    R : 1d array\n",
    "      Reward earned.\n",
    "    n_states : int\n",
    "      Total number of unique states in task.\n",
    "    n_actions : int\n",
    "      Total number of unique actions in task.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    log_lik : scalar\n",
    "      Negative log-likelihood of data given parameters.\n",
    "      \n",
    "    Notes\n",
    "    -----\n",
    "    The choice data should be a 2d matrix of size [N,M]\n",
    "    where N is the number of trials and M is the number\n",
    "    of states visited per trial. Matrix should be pythonic\n",
    "    such that first state is s=0.\n",
    "    '''\n",
    "    \n",
    "    ## Extract parameters.\n",
    "    eta, lambd, beta = params\n",
    "    \n",
    "    ## Transform into proper units.\n",
    "    eta = phi_approx(eta)\n",
    "    lambd = phi_approx(lambd)\n",
    "    beta = phi_approx(beta) * 10\n",
    "    \n",
    "    ## Initialize Q-values.\n",
    "    Q = np.zeros((n_states, n_actions))\n",
    "    \n",
    "    log_lik = 0\n",
    "    for i in np.arange(R.size):\n",
    "                  \n",
    "        ## Compute likelihood of choice at first state (s).\n",
    "        s = 0\n",
    "        theta = softmax(Q[s], beta)\n",
    "        log_lik += np.log(theta[Y[i,0]])\n",
    "        \n",
    "        ## Compute likelihood of choice at second state (s_prime).\n",
    "        s_prime = Y[i,0] + 1\n",
    "        theta = softmax(Q[s_prime], beta)\n",
    "        log_lik += np.log(theta[Y[i,1]])\n",
    "        \n",
    "        ## Compute reward prediction error (delta).\n",
    "        delta = R[i] - Q[s_prime, Y[i,1]]\n",
    "        \n",
    "        ## Compute eligibility traces.\n",
    "        E = np.zeros_like(Q)\n",
    "        E[s,Y[i,0]] = lambd\n",
    "        E[s_prime,Y[i,1]] = 1\n",
    "        \n",
    "        ## Update Q-values.\n",
    "        Q += eta * delta * E\n",
    "        \n",
    "    return -log_lik"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model-Based Learning\n",
    "\n",
    "1) At the final, or terminal, states we know that there is no next state-action pair. Thus, the formula simplifies to:\n",
    "\n",
    "$$ \\delta = R - Q(s,a) $$\n",
    "\n",
    "$$ Q(s,a) = Q(s,a) + \\eta \\delta $$\n",
    "\n",
    "2) At the initial state we have:\n",
    "\n",
    "$$ Q(s_1,a_1) = p(s_2 \\mid s_1, a_1) \\max_{a \\in a_1, a_2} Q(s_2,a) + p(s_3 \\mid s_1, a_1) \\max_{a \\in a_1, a_2} Q(s_3,a) $$\n",
    "\n",
    "$$ Q(s_1,a_2) = p(s_2 \\mid s_1, a_2) \\max_{a \\in a_1, a_2} Q(s_2,a) + p(s_3 \\mid s_1, a_2) \\max_{a \\in a_1, a_2} Q(s_3,a) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mb_ngl(params, Y, R, n_states=3, n_actions=2):\n",
    "    '''Negative log-likelihood function of model-based learning.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    params : list\n",
    "      Model parameters (eta, beta).\n",
    "    Y : 2d array\n",
    "      Choices of participant (see notes).\n",
    "    R : 1d array\n",
    "      Reward earned.\n",
    "    n_states : int\n",
    "      Total number of unique states in task.\n",
    "    n_actions : int\n",
    "      Total number of unique actions in task.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    log_lik : scalar\n",
    "      Negative log-likelihood of data given parameters.\n",
    "      \n",
    "    Notes\n",
    "    -----\n",
    "    The choice data should be a 2d matrix of size [N,M]\n",
    "    where N is the number of trials and M is the number\n",
    "    of states visited per trial. Matrix should be pythonic\n",
    "    such that first state is s=0.\n",
    "    '''\n",
    "    \n",
    "    ## Extract parameters.\n",
    "    eta, beta = params\n",
    "    \n",
    "    ## Transform into proper units.\n",
    "    eta = phi_approx(eta)\n",
    "    beta = phi_approx(beta) * 10\n",
    "    \n",
    "    ## Initialize Q-values.\n",
    "    Q = np.zeros((n_states, n_actions))\n",
    "    \n",
    "    log_lik = 0\n",
    "    for i in np.arange(R.size):\n",
    "                  \n",
    "        ## Compute likelihood of choice at first state (s).\n",
    "        s = 0\n",
    "        theta = softmax(Q[s], beta)\n",
    "        log_lik += np.log(theta[Y[i,0]])\n",
    "        \n",
    "        ## Compute likelihood of choice at second state (s_prime).\n",
    "        s_prime = Y[i,0] + 1\n",
    "        theta = softmax(Q[s_prime], beta)\n",
    "        log_lik += np.log(theta[Y[i,1]])\n",
    "        \n",
    "        ## Compute reward prediction error (delta).\n",
    "        delta = R[i] - Q[s_prime, Y[i,1]]        \n",
    "        \n",
    "        ## Update Q-values.\n",
    "        Q[s_prime, Y[i,1]] += eta * delta                    # Terminal state\n",
    "        Q[0, 0] = 0.7 * np.max(Q[1]) + 0.3 * np.max(Q[2])    # s1, a1\n",
    "        Q[0, 1] = 0.3 * np.max(Q[1]) + 0.7 * np.max(Q[2])    # s1, a2\n",
    "        \n",
    "    return -log_lik"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2\n",
    "### Load and prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load matlab file (Paula's data).\n",
    "mat = loadmat('Subj100_2018-4-20_11-1-37.mat')\n",
    "# mat = loadmat('data.mat')\n",
    "\n",
    "## Assemble choice data.\n",
    "Y = np.array([mat['choice1'].squeeze(), \n",
    "              mat['choice2'].squeeze()]).T\n",
    "Y -= 1\n",
    "\n",
    "## Assemble reward data.\n",
    "R = mat['money'].squeeze()\n",
    "\n",
    "## Remove trials with missing data.\n",
    "indices = np.invert(np.any(Y<0, axis=-1))\n",
    "Y = Y[indices]\n",
    "R = R[indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model-Free Learning with SARSA-TD($\\lambda$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Convergence = True\n",
      "Estimated Parameters\n",
      "eta = 0.244\n",
      "lambda = 0.494\n",
      "beta = 1.896\n"
     ]
    }
   ],
   "source": [
    "## Define initial parameters.\n",
    "x0 = np.zeros(3, dtype=float)\n",
    "\n",
    "## Minimize negative log-likelihood.\n",
    "fit = minimize(sarsa_ngl, x0, args=(Y,R))\n",
    "print('Convergence = %s' %fit.success)\n",
    "\n",
    "## Extract parameters.\n",
    "eta, lambd, beta = phi_approx(fit.x)\n",
    "beta *= 10\n",
    "print('Estimated Parameters')\n",
    "print('eta = %0.3f' %eta)\n",
    "print('lambda = %0.3f' %lambd)\n",
    "print('beta = %0.3f' %beta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model-Based Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Convergence = False\n",
      "Estimated Parameters\n",
      "eta = 0.257\n",
      "beta = 1.939\n"
     ]
    }
   ],
   "source": [
    "## Define initial parameters.\n",
    "x0 = np.zeros(2, dtype=float)\n",
    "\n",
    "## Minimize negative log-likelihood.\n",
    "fit = minimize(mb_ngl, x0, args=(Y,R))\n",
    "print('Convergence = %s' %fit.success)\n",
    "\n",
    "## Extract parameters.\n",
    "eta, beta = phi_approx(fit.x)\n",
    "beta *= 10\n",
    "print('Estimated Parameters')\n",
    "print('eta = %0.3f' %eta)\n",
    "print('beta = %0.3f' %beta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix: Stan Implementations\n",
    "### Model-Free Learning with SARSA-TD($\\lambda$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pystan:COMPILING THE C++ CODE FOR MODEL anon_model_3e098205698f4bfaf54ae1d60e6439d9 NOW.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference for Stan model: anon_model_3e098205698f4bfaf54ae1d60e6439d9.\n",
      "4 chains, each with iter=1250; warmup=1000; thin=1; \n",
      "post-warmup draws per chain=250, total post-warmup draws=1000.\n",
      "\n",
      "            mean se_mean     sd   2.5%    25%    50%    75%  97.5%  n_eff   Rhat\n",
      "eta_pr     -0.39    0.03   0.52  -1.19  -0.75  -0.47  -0.08   0.82    352    1.0\n",
      "lambda_pr   0.32    0.04   0.72  -1.03  -0.16   0.25   0.74   2.06    372    1.0\n",
      "beta_pr    -1.08    0.02   0.29  -1.66  -1.27  -1.08  -0.87  -0.51    293    1.0\n",
      "eta         0.36  9.3e-3   0.18   0.12   0.23   0.32   0.47   0.79    358    1.0\n",
      "lambda       0.6    0.01   0.22   0.15   0.44    0.6   0.77   0.98    393    1.0\n",
      "beta        1.51    0.04   0.66   0.48   1.02   1.41   1.92   3.05    284    1.0\n",
      "lp__      -272.9    0.07   1.33 -276.5 -273.4 -272.6 -272.0 -271.5    338    1.0\n",
      "\n",
      "Samples were drawn using NUTS at Sat Apr 21 19:34:41 2018.\n",
      "For each parameter, n_eff is a crude measure of effective sample size,\n",
      "and Rhat is the potential scale reduction factor on split chains (at \n",
      "convergence, Rhat=1).\n"
     ]
    }
   ],
   "source": [
    "try: \n",
    "    import pystan\n",
    "except ModuleNotFoundError: \n",
    "    pass\n",
    "else:\n",
    "\n",
    "    ## Compile Model.\n",
    "    StanMF = pystan.StanModel(file='stan_models/sarsa.stan')\n",
    "    \n",
    "    ## Fit model.\n",
    "    data = dict(N=R.size, Y=Y+1, R=R)\n",
    "    fit = StanMF.sampling(data=data, chains=4, iter=1250, \n",
    "                          warmup=1000, seed=47404, n_jobs=4)\n",
    "    \n",
    "    print(fit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model-Based Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pystan:COMPILING THE C++ CODE FOR MODEL anon_model_cab21fa26a61287d8736a1f9c7bdd386 NOW.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference for Stan model: anon_model_cab21fa26a61287d8736a1f9c7bdd386.\n",
      "4 chains, each with iter=1250; warmup=1000; thin=1; \n",
      "post-warmup draws per chain=250, total post-warmup draws=1000.\n",
      "\n",
      "          mean se_mean     sd   2.5%    25%    50%    75%  97.5%  n_eff   Rhat\n",
      "eta_pr   -0.46    0.02   0.42  -1.11  -0.72  -0.53  -0.28   0.64    351    1.0\n",
      "beta_pr  -1.01    0.02   0.31  -1.62  -1.17  -0.98  -0.81   -0.5    249   1.01\n",
      "eta       0.33  7.8e-3   0.15   0.13   0.23    0.3   0.39   0.74    354    1.0\n",
      "beta      1.67    0.04   0.67   0.52   1.21   1.64    2.1   3.07    291   1.01\n",
      "lp__    -274.5     0.1   1.35 -277.7 -274.9 -274.1 -273.6 -273.3    179   1.01\n",
      "\n",
      "Samples were drawn using NUTS at Sat Apr 21 19:35:48 2018.\n",
      "For each parameter, n_eff is a crude measure of effective sample size,\n",
      "and Rhat is the potential scale reduction factor on split chains (at \n",
      "convergence, Rhat=1).\n"
     ]
    }
   ],
   "source": [
    "try: \n",
    "    import pystan\n",
    "except ModuleNotFoundError: \n",
    "    pass\n",
    "else:\n",
    "\n",
    "    ## Compile Model.\n",
    "    StanMB = pystan.StanModel(file='stan_models/mb.stan')\n",
    "    \n",
    "    ## Fit model.\n",
    "    data = dict(N=R.size, Y=Y+1, R=R)\n",
    "    fit = StanMB.sampling(data=data, chains=4, iter=1250, \n",
    "                          warmup=1000, seed=47404, n_jobs=4)\n",
    "    \n",
    "    print(fit)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
